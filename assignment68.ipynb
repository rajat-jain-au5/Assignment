{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69ebd9-5958-48c9-a208-a2ae83d3305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "ans:\n",
    "Gradient Boosting Regression is a machine learning algorithm used for regression problems. It is an ensemble learning method that combines multiple weak regression \n",
    "models to create a more accurate and robust model.\n",
    "\n",
    "In Gradient Boosting Regression, the weak models are decision trees, which are fitted sequentially to the training data, where each subsequent tree is trained on \n",
    "the residuals of the previous tree. The goal of the algorithm is to minimize the mean squared error (MSE) between the predicted and actual target values.\n",
    "\n",
    "At each stage, the algorithm computes the negative gradient of the loss function with respect to the predicted values, which serves as the target for the next \n",
    "decision tree. The new decision tree is then fit to the negative gradient residuals, where the objective is to predict the negative gradient residuals as accurately\n",
    "as possible. The predicted negative gradient residuals are added to the previous predictions, creating a new and more accurate prediction. This process is repeated \n",
    "until the desired level of accuracy is achieved or a predefined number of trees have been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9910674f-99e0-4771-983b-a1885bf7b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 23504.67907244988\n",
      "R-Squared: 0.7715740568371017\n"
     ]
    }
   ],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "# simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "# performance using metrics such as mean squared error and R-squared.\n",
    "# Sure, I can provide an example implementation of Gradient Boosting Regression using Python and NumPy. Here's an example code that uses a simple linear regression \n",
    "# problem and the Boston Housing dataset:\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.mean = np.mean(y)\n",
    "        y_pred = np.full(y.shape, self.mean)\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            residuals = y - y_pred\n",
    "            tree.fit(X, residuals)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.mean)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Example usage on a small dataset\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-Squared:\", r_squared(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99d7ef-5592-4e57-b740-42481f97224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "# optimise the performance of the model. Use grid search or random search to find the best\n",
    "# hyperparameters\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Perform a grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and corresponding metrics\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", -grid_search.best_score_)\n",
    "print(\"RMSE: \", mean_squared_error(y, grid_search.predict(X), squared=False))\n",
    "print(\"R-squared: \", r2_score(y, grid_search.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a4600-262e-4956-9451-09cbdf69bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "ans:\n",
    "A weak learner in Gradient Boosting is a simple model or an algorithm that performs slightly better than random guessing on a given dataset. In the context of Gradient Boosting,\n",
    "a weak learner is typically a decision tree with a small number of nodes or depth.\n",
    "\n",
    "The weak learner's job is to identify patterns in the training data that can be used to make better predictions. In Gradient Boosting, the weak learner is trained on the \n",
    "residuals or errors of the previous model, which allows it to focus on the difficult examples that the previous model got wrong. The idea is that by combining multiple weak \n",
    "learners, each one focusing on a different subset of the data, we can create a stronger model that is able to make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb33179-0a4c-4e4f-b7a9-3f3aaf485345",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "ans:\n",
    "The intuition behind the Gradient Boosting algorithm is to build a strong model by sequentially adding weak models that are trained to correct the errors of the previous models.\n",
    "The idea is that each model learns from the errors of the previous models, and focuses on the examples that were difficult to predict.\n",
    "\n",
    "The Gradient Boosting algorithm works by initially fitting a simple model to the data, such as a decision tree with a small number of nodes. This model is trained to predict the \n",
    "target variable as accurately as possible. Then, the algorithm calculates the residuals or errors of the predictions made by the first model. These residuals represent the \n",
    "difference between the actual values and the predictions made by the first model.\n",
    "\n",
    "Next, the algorithm trains a second model, which is focused on predicting the residuals of the first model. This second model is also a weak learner, and is typically a decision \n",
    "tree with a small number of nodes. The second model is trained to minimize the errors or residuals of the first model. The predictions of the first and second model are then \n",
    "added together to obtain a better estimate of the target variable.\n",
    "\n",
    "This process is repeated for a predefined number of iterations, with each subsequent model focusing on the residuals of the previous models. By combining multiple weak models\n",
    "that correct the errors of the previous models, the algorithm is able to create a strong model that is able to make accurate predictions on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4e862-233d-4beb-82fc-f85cbff724c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "ans:\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. The idea is to train a series of weak models or learners that focus on correcting \n",
    "the errors made by the previous models. Here are the steps involved in building an ensemble of weak learners using Gradient Boosting:\n",
    "\n",
    "Initialize the prediction: In the beginning, the prediction is initialized to a constant value, such as the mean of the target variable.\n",
    "\n",
    "Train the first weak learner: A simple model, such as a decision tree with a small number of nodes, is trained on the input features and the difference between the actual values \n",
    "and the initial prediction. The goal is to minimize the residual error between the actual and predicted values.\n",
    "\n",
    "Update the prediction: The predictions of the first weak learner are added to the initial prediction, creating a new prediction. This new prediction is used as the target \n",
    "variable for the next weak learner.\n",
    "\n",
    "Train the next weak learner: A new weak learner is trained on the input features and the difference between the actual values and the updated prediction. The goal is to minimize\n",
    "\n",
    "the residual error between the actual and updated predicted values.\n",
    "\n",
    "Update the prediction again: The predictions of the latest weak learner are added to the updated prediction, creating a new prediction. This new prediction is used as the target\n",
    "variable for the next weak learner.\n",
    "\n",
    "Repeat steps 4 and 5: Steps 4 and 5 are repeated for a fixed number of iterations or until the error stops decreasing.\n",
    "\n",
    "Combine the weak learners: The final prediction is obtained by combining the predictions of all the weak learners. This is done by adding the weighted predictions of each weak \n",
    "learner, where the weights are determined by the performance of each learner.\n",
    "\n",
    "The idea behind Gradient Boosting is to iteratively fit a sequence of weak models to the residuals of the previous model. By combining the predictions of these weak models, the \n",
    "algorithm is able to build a strong ensemble model that is able to make accurate predictions on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db4e34-91d0-4e71-849c-1558d62dd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "ans:\n",
    "The mathematical intuition behind Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "Initialize the prediction: In the beginning, the prediction is initialized to a constant value, such as the mean of the target variable.\n",
    "\n",
    "Define the loss function: The loss function is used to measure the difference between the predicted and actual values. The goal is to minimize the loss function by adjusting the\n",
    "model parameters.\n",
    "\n",
    "Train the first weak learner: A simple model, such as a decision tree with a small number of nodes, is trained on the input features and the difference between the actual values\n",
    "and the initial prediction. The goal is to minimize the loss function between the actual and predicted values.\n",
    "\n",
    "Update the prediction: The predictions of the first weak learner are added to the initial prediction, creating a new prediction. This new prediction is used as the target \n",
    "variable for the next weak learner.\n",
    "\n",
    "Train the next weak learner: A new weak learner is trained on the input features and the difference between the actual values and the updated prediction. The goal is to minimize\n",
    "the loss function between the actual and updated predicted values.\n",
    "\n",
    "Update the prediction again: The predictions of the latest weak learner are added to the updated prediction, creating a new prediction. This new prediction is used as the target \n",
    "variable for the next weak learner.\n",
    "\n",
    "Repeat steps 5 and 6: Steps 5 and 6 are repeated for a fixed number of iterations or until the loss stops decreasing.\n",
    "\n",
    "Combine the weak learners: The final prediction is obtained by combining the predictions of all the weak learners. This is done by adding the weighted predictions of each weak \n",
    "learner, where the weights are determined by the performance of each learner.\n",
    "\n",
    "The mathematical intuition behind Gradient Boosting involves optimizing the loss function by iteratively adding new weak models to the ensemble. Each new model is trained on\n",
    "the residuals of the previous model, allowing the algorithm to correct the errors made by the previous model. The final prediction is obtained by combining the predictions of\n",
    "all the weak models in the ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
