{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09703c55-eecc-44ac-9324-e528be127e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "ans:\n",
    "Batch normalization is a technique used in Artificial Neural Networks (ANNs) to improve the training and performance of deep learning models. It addresses the internal covariate\n",
    "shift problem, which refers to the change in the distribution of the input values to layers within a neural network during training.\n",
    "\n",
    "In a neural network, the distribution of the input data to each layer can change as the parameters of the preceding layers get updated. This makes the training process slower \n",
    "and more challenging since the subsequent layers have to constantly adapt to the changing input distribution. Batch normalization helps to mitigate this issue by normalizing \n",
    "the inputs to each layer.\n",
    "\n",
    "The main idea behind batch normalization is to normalize the inputs of a layer by subtracting the mean and dividing by the standard deviation of the inputs within a mini-batch \n",
    "of training examples. The normalization is applied independently to each feature dimension, effectively transforming the input data to have zero mean and unit variance.\n",
    "\n",
    "During training, batch normalization introduces two additional learnable parameters per feature dimension: a scale parameter and a shift parameter. These parameters allow the\n",
    "model to learn the optimal scale and shift for each feature, effectively recovering the representation power of the original input.\n",
    "\n",
    "The batch normalization process can be summarized in the following steps:\n",
    "\n",
    "Compute the mean and standard deviation of the inputs within a mini-batch.\n",
    "Normalize the inputs by subtracting the mean and dividing by the standard deviation.\n",
    "Scale and shift the normalized inputs using learnable parameters.\n",
    "Pass the scaled and shifted inputs through the activation function of the layer.\n",
    "Update the scale and shift parameters during the training process using gradient descent.\n",
    "By normalizing the inputs within each mini-batch, batch normalization helps to reduce the internal covariate shift. It has several benefits, including faster convergence during \n",
    "training, increased stability, and the ability to use higher learning rates. Moreover, batch normalization acts as a form of regularization, reducing the dependence on dropout \n",
    "or other regularization techniques.\n",
    "\n",
    "During inference or evaluation, the mean and standard deviation used for normalization are typically computed using the population statistics from the entire training dataset or\n",
    "a moving average over multiple mini-batches.\n",
    "\n",
    "Overall, batch normalization has become a widely adopted technique in deep learning as it facilitates the training of deep neural networks and improves their generalization \n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cdd73-c8b6-484d-bd7c-cf3f8ab68467",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 Describe the benefits of using batch normalization during training,\n",
    "ans:\n",
    "Batch normalization offers several benefits when used during the training of neural networks:\n",
    "\n",
    "Accelerated training convergence: Batch normalization helps in reducing the number of training iterations required for a model to converge. By normalizing the inputs, it ensures\n",
    "that the subsequent layers receive inputs with a more consistent distribution, which in turn helps the gradients flow more smoothly through the network. This leads to faster\n",
    "convergence and shorter training times.\n",
    "\n",
    "\n",
    "Mitigation of internal covariate shift: The internal covariate shift occurs when the distribution of input values to each layer of the network changes during training. Batch \n",
    "normalization combats this issue by normalizing the inputs within each mini-batch. This stabilizes the distribution of the inputs, making it easier for the subsequent layers to\n",
    "learn and adapt to the changing data distribution.\n",
    "\n",
    "Increased stability and higher learning rates: Batch normalization helps to stabilize the training process by reducing the sensitivity of the network to the initial weights and\n",
    "hyperparameters. It allows for the use of higher learning rates without the risk of the network diverging. This is particularly beneficial when training deep neural networks \n",
    "where convergence can be challenging.\n",
    "\n",
    "Regularization effect: Batch normalization acts as a form of regularization. By normalizing the inputs and adding learnable scale and shift parameters, it introduces noise to \n",
    "the network during training. This noise helps to reduce overfitting and makes the model more robust to small variations in the input data.\n",
    "\n",
    "Gradient propagation improvement: Normalizing the inputs using batch normalization helps to alleviate the vanishing or exploding gradient problems in deep networks. By \n",
    "maintaining a more consistent range of values for the inputs, it ensures that the gradients are neither too small nor too large, allowing for more stable and effective gradient\n",
    "propagation through the network.\n",
    "\n",
    "Reducing the dependence on dropout: Dropout is a commonly used regularization technique that randomly sets a fraction of the activations to zero during training. Batch \n",
    "normalization reduces the reliance on dropout by providing some regularization effect on its own. This can lead to simpler and more efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaadc8-0cbf-43e7-851a-d34d024d258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters.\n",
    "ans:\n",
    "\n",
    "The working principle of batch normalization involves two main steps: the normalization step and the introduction of learnable parameters. Let's discuss each \n",
    "of these steps in detail:\n",
    "\n",
    "Normalization Step:\n",
    "\n",
    "Batch normalization operates on a mini-batch of training examples. For each feature dimension, it computes the mean and standard deviation of the inputs\n",
    "within the mini-batch.\n",
    "The mean is calculated by averaging the values of the feature dimension across the mini-batch, and the standard deviation is calculated using the mean as\n",
    "the reference.\n",
    "Then, it subtracts the mean from each input and divides the result by the standard deviation. This step ensures that the inputs have zero mean and unit \n",
    "variance.\n",
    "The normalization is applied independently to each feature dimension, preserving the correlations among the features.\n",
    "\n",
    "Introduction of Learnable Parameters:\n",
    "\n",
    "After normalization, batch normalization introduces two learnable parameters per feature dimension: scale and shift.\n",
    "The scale parameter (gamma) and shift parameter (beta) are learnable parameters that allow the model to learn the optimal scale and shift for each feature\n",
    "dimension.\n",
    "The scaled and shifted inputs are obtained by multiplying the normalized inputs by the scale parameter and adding the shift parameter.\n",
    "These parameters are initialized to 1 and 0, respectively, but they are updated during the training process through backpropagation.\n",
    "The introduction of these learnable parameters allows the model to recover the representation power of the original input. By scaling and shifting the \n",
    "normalized inputs, the\n",
    "model can learn to undo the normalization if it finds it necessary for the optimal representation of the data. The scale parameter allows the model to control \n",
    "the spread of the \n",
    "feature values, while the shift parameter allows it to control the mean.\n",
    "\n",
    "During training, the parameters (scale and shift) of batch normalization are updated through gradient descent along with the other parameters of the neural \n",
    "network. The \n",
    "gradients for the scale and shift parameters are computed using the chain rule during backpropagation, and the optimizer updates these parameters accordingly.\n",
    "\n",
    "It's important to note that during inference or evaluation, batch normalization uses the population statistics (mean and standard deviation) computed from the\n",
    "entire training \n",
    "dataset or a moving average over multiple mini-batches. This ensures consistent normalization for new, unseen examples.\n",
    "\n",
    "Overall, the working principle of batch normalization involves normalizing the inputs within each mini-batch to have zero mean and unit variance, and then \n",
    "introducing learnable \n",
    "parameters (scale and shift) to allow the model to adapt and control the normalization for optimal representation and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adc21a-57c9-408e-80a5-f1348a030166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementation\n",
    "Q1 Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "Q2 Implement a simple feedforward neural network using any deep learning framework/library (e.g. Tensorlow, pyTorch)\n",
    "Q3 Train the neural network on the chosen dataset without using batch normalization.\n",
    "Q4 Implement batch normalization layers in the neural network and train the model again.\n",
    "Q5 Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "Q6 Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5366ec83-8a32-4834-9b85-ea0eccd3c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 05:29:12.856700: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-15 05:29:12.923892: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-15 05:29:12.925004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-15 05:29:14.083752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a73d61-6cad-4051-b646-575740903d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c29b69-2859-4ed0-b436-60d399b8f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# : Implementing a Simple Feedforward Neural Network\n",
    "# Define the model architecture\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24264e42-a14f-4b87-8f1b-dffa5c590e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2377 - accuracy: 0.9295 - val_loss: 0.1221 - val_accuracy: 0.9605\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0997 - accuracy: 0.9686 - val_loss: 0.0893 - val_accuracy: 0.9732\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0678 - accuracy: 0.9786 - val_loss: 0.0835 - val_accuracy: 0.9740\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0520 - accuracy: 0.9840 - val_loss: 0.0744 - val_accuracy: 0.9778\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0411 - accuracy: 0.9865 - val_loss: 0.0811 - val_accuracy: 0.9759\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0325 - accuracy: 0.9897 - val_loss: 0.0869 - val_accuracy: 0.9770\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0288 - accuracy: 0.9905 - val_loss: 0.0862 - val_accuracy: 0.9758\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0246 - accuracy: 0.9919 - val_loss: 0.0975 - val_accuracy: 0.9751\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.0846 - val_accuracy: 0.9794\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.1009 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efd8aab6f50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Neural Network without Batch Normalization\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408f347f-8a6b-4c1b-b7bf-ac1ba9094856",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Implementing Batch Normalization Layers\n",
    "model_with_batchnorm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with batch normalization\n",
    "model_with_batchnorm.compile(optimizer='adam',\n",
    "                             loss='sparse_categorical_crossentropy',\n",
    "                             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55050899-b6db-4ac1-86e6-65ea8f37d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0354 - accuracy: 0.9880 - val_loss: 0.0694 - val_accuracy: 0.9823\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.0666 - val_accuracy: 0.9806\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0304 - accuracy: 0.9899 - val_loss: 0.0731 - val_accuracy: 0.9806\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0273 - accuracy: 0.9908 - val_loss: 0.0685 - val_accuracy: 0.9812\n",
      "Epoch 5/10\n",
      "1126/1875 [=================>............] - ETA: 2s - loss: 0.0250 - accuracy: 0.9918"
     ]
    }
   ],
   "source": [
    " # Comparing Training and Validation Performance\n",
    "model_with_batchnorm.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce75c1-4ece-4530-b422-66668d39249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model without batch normalization\n",
    "_, acc_no_bn = model.evaluate(x_test, y_test)\n",
    "print(\"Model without Batch Normalization - Test Accuracy:\", acc_no_bn)\n",
    "\n",
    "# Evaluate the model with batch normalization\n",
    "_, acc_with_bn = model_with_batchnorm.evaluate(x_test, y_test)\n",
    "print(\"Model with Batch Normalization - Test Accuracy:\", acc_with_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa56c6d-d376-4023-8008-69d441f1f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Impact of Batch Normalization\n",
    "Batch normalization has several positive impacts on the training process and the performance of a neural network:\n",
    "\n",
    "Improved convergence speed: Batch normalization helps in faster convergence by reducing internal covariate shift. It normalizes the activations of each batch, reducing the scale and shifting of the input distributions.\n",
    "Reduced sensitivity to weight initialization: Batch normalization reduces the dependence of the model's performance on the initial weights. It allows the use of higher learning rates without the risk of divergence.\n",
    "Regularization effect: Batch normalization adds a slight regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
