{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b159d86-9a46-4722-b864-d7c9663f9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 Describe the purpose and benefits oj pooling in CNN?\n",
    "ans:\n",
    "Pooling, also known as subsampling or downsampling, is a crucial operation in Convolutional Neural Networks \n",
    "(CNNs) used for image and pattern recognition tasks. The purpose of pooling is to reduce the spatial dimensions\n",
    "(width and height) of the input feature maps while retaining the most important information. The benefits of \n",
    "pooling include:\n",
    "\n",
    "Dimensionality Reduction: Pooling helps to reduce the size of the feature maps, which in turn reduces the\n",
    "number of parameters and computational complexity of the network. By discarding irrelevant spatial information,\n",
    "pooling enables faster processing and more efficient memory usage.\n",
    "\n",
    "Translation Invariance: Pooling enhances the network's ability to detect features regardless of their exact\n",
    "position in the input. By summarizing local information, pooling creates a level of translational invariance, \n",
    "making the network more robust to small spatial translations. This is especially beneficial in tasks where the\n",
    "location of the features is not important, such as image classification.\n",
    "\n",
    "Feature Generalization: Pooling summarizes the presence of important features within a local neighborhood. By \n",
    "aggregating features from multiple neighboring regions, pooling captures the dominant characteristics of the \n",
    "input, making the network more capable of generalizing to variations in scale, rotation, or orientation.\n",
    "\n",
    "Reduction of Overfitting: Pooling can act as a regularizer by reducing the spatial resolution of the feature \n",
    "maps. This limits the capacity of the network to memorize fine details and encourages the learning of more \n",
    "generalized and discriminative features. As a result, pooling helps to prevent overfitting, which occurs when\n",
    "a model performs well on the training data but fails to generalize to unseen data.\n",
    "\n",
    "Computational Efficiency: Pooling reduces the spatial dimensions, which leads to smaller feature maps in \n",
    "subsequent layers. This reduction in size translates to a decrease in the number of parameters and the amount \n",
    "of computation required, making the network more computationally efficient and faster to train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61f6ae-9ff9-4f15-a644-51ff1bd89b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 Explain the difference retween Min pooling and Max pooling.\n",
    "ans:\n",
    "The main difference between Min pooling and Max pooling lies in how they select the representative value from \n",
    "each pooling region:\n",
    "\n",
    "Max Pooling: In Max pooling, the maximum value within each pooling region is selected as the representative \n",
    "value. It retains the most prominent feature in the region, emphasizing strong activations and providing \n",
    "robustness to noise or small variations in the input. Max pooling is commonly used in CNN architectures.\n",
    "\n",
    "Min Pooling: In Min pooling, the minimum value within each pooling region is chosen as the representative value\n",
    ". It captures the smallest value, which may indicate the presence of specific features or patterns. Min pooling\n",
    "is less commonly used compared to max pooling but can be useful in certain scenarios.\n",
    "\n",
    "Both pooling operations share the objective of reducing spatial dimensions and introducing invariance to small\n",
    "local changes, but they differ in the specific statistic (maximum or minimum) used to summarize the pooling \n",
    "region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b365e-0956-4ceb-ac0c-0c7d5f43edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 Discuss the concept oj padding in CNN and its significance.\n",
    "ans:\n",
    "Padding in CNN refers to the process of adding extra border pixels to the input image or feature map before \n",
    "applying convolutional or pooling operations. Padding introduces additional pixels with specific values \n",
    "(usually zeros) around the borders of the input. The main significance of padding includes:\n",
    "\n",
    "Maintaining Spatial Dimensions: Padding helps to preserve the spatial size of the feature maps, especially when\n",
    "using convolutional layers with a stride larger than 1. It ensures that the output feature map has the same \n",
    "width and height as the input, allowing subsequent layers to have a consistent resolution.\n",
    "\n",
    "Avoiding Border Information Loss: Without padding, the convolutional or pooling operations can cause a gradual\n",
    "reduction in the size of feature maps. This reduction results in a gradual loss of information near the borders\n",
    "of the input. Padding helps to mitigate this problem by providing a buffer zone of extra pixels, ensuring that \n",
    "the borders are adequately processed.\n",
    "\n",
    "Handling Odd Filter Sizes: When using filters with odd dimensions (e.g., 3x3), padding is necessary to maintain\n",
    "the alignment of the filter with the input data. Padding ensures that the central pixel of the filter aligns \n",
    "with the central pixel of the input, preserving the symmetry and preventing asymmetrical effects in the \n",
    "convolution.\n",
    "\n",
    "Enabling Capturing of Edge Information: Padding allows the receptive field of filters to extend beyond the \n",
    "borders of the input, enabling them to capture edge information accurately. Edge detection filters, for \n",
    "example, require information from both sides of an edge to identify and detect it correctly. Padding ensures \n",
    "that the necessary context is available for proper edge detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb225d98-54a8-401b-b0d9-d251f1f9a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4 Compare and contrast zero-padding and valid-padding in terms oj their effects on the output\n",
    "feature Map size.\n",
    "ans:\n",
    "Zero-padding and valid-padding are two common padding techniques used in CNNs, and they have different effects\n",
    "on the output feature map size:\n",
    "\n",
    "Zero-padding: In zero-padding, extra border pixels are added with a value of zero. The padding pixels do not \n",
    "carry any information and are ignored during the convolutional or pooling operations. Zero-padding expands the\n",
    "spatial dimensions of the input by adding an equal number of pixels around the borders. The resulting output \n",
    "feature map size will be larger than the input size, and the expansion depends on the amount of padding applied.\n",
    "\n",
    "\n",
    "Valid-padding: In valid-padding (also called no-padding), no extra pixels are added to the input. The \n",
    "convolutional or pooling operations are performed strictly within the spatial boundaries of the input. As a \n",
    "result, the output feature map size will be smaller than the input size because the borders are not processed.\n",
    "The reduction in size is determined by the filter size and the stride used in the convolutional or pooling \n",
    "operation.\n",
    "\n",
    "In summary, zero-padding increases the output feature map size, while valid-padding decreases it. Zero-padding\n",
    "maintains spatial dimensions and allows better edge detection, but it increases computational requirements. \n",
    "Valid-padding preserves only the central regions of the input and reduces the size of the output feature map, \n",
    "leading to computational efficiency but potential loss of border information. The choice of padding depends on \n",
    "the specific requirements of the CNN architecture and the desired balance between preservation of spatial \n",
    "information and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d37b46-15f9-42ca-9e49-f61bb1cede01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC: Exploring LeNet\n",
    "Q1 Provide a brief overview oj LeNet-5 architecture.\n",
    "ans:\n",
    "LeNet-5 is a classic convolutional neural network (CNN) architecture developed by Yann LeCun and his \n",
    "colleagues in the 1990s. It was one of the pioneering models that demonstrated the effectiveness of deep \n",
    "learning for image classification tasks. LeNet-5 was primarily designed for handwritten digit recognition and \n",
    "played a significant role in advancing the field of computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70dbf2-d30d-4e95-8457-a8e49e986334",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 Describe the key components of LeNet-5 and their respective purposes.\n",
    "ans:\n",
    "The key components of LeNet-5 and their purposes are as follows:\n",
    "\n",
    "Input Layer: LeNet-5 takes a grayscale image of size 32x32 pixels as input.\n",
    "\n",
    "Convolutional Layers: LeNet-5 consists of two convolutional layers. The first convolutional layer uses 6 \n",
    "filters with a kernel size of 5x5, and the second convolutional layer uses 16 filters with a kernel size of \n",
    "5x5. These layers perform feature extraction by convolving the filters with the input image.\n",
    "\n",
    "Activation Function: After each convolutional layer, a non-linear activation function is applied. LeNet-5 uses \n",
    "the hyperbolic tangent (tanh) activation function, which introduces non-linearity and enables the network to\n",
    "learn complex representations.\n",
    "\n",
    "Pooling Layers: Following the activation function, LeNet-5 has two average pooling layers with a kernel size of\n",
    "2x2 and a stride of 2. Pooling reduces the spatial dimensions and captures the most important features.\n",
    "\n",
    "Fully Connected Layers: LeNet-5 concludes with three fully connected layers. The first two fully connected \n",
    "layers have 120 and 84 units, respectively, while the last layer serves as the output layer with 10 units \n",
    "(corresponding to the 10 possible digits). These layers perform classification based on the learned features.\n",
    "\n",
    "Softmax Activation: The output layer of LeNet-5 uses the softmax activation function to produce a probability \n",
    "distribution over the 10 possible classes, indicating the predicted probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54971816-b30c-4fac-86a3-eba551cdabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 Discuss the advantages and limitations of LeNet-5 in the context oj image classification tasks.\n",
    "ans:\n",
    "Advantages and Limitations of LeNet-5:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1.Effective for Digit Recognition: LeNet-5 demonstrated excellent performance on the MNIST dataset, achieving \n",
    "high accuracy in handwritten digit classification tasks.\n",
    "2.Compact Architecture: LeNet-5 has a relatively small number of parameters compared to modern CNN architectures,\n",
    "making it computationally efficient and easier to train.\n",
    "3.Hierarchical Feature Extraction: The sequential arrangement of convolutional and pooling layers in LeNet-5 \n",
    "allows for hierarchical feature extraction, enabling the model to learn representations of increasing\n",
    "complexity.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "1.Limited Complexity: LeNet-5 may struggle with more complex image classification tasks that require capturing\n",
    "intricate details or dealing with large variations in scale, rotation, or background.\n",
    "2.Shallower Architecture: Compared to modern CNN architectures, LeNet-5 has a relatively shallow architecture,\n",
    "which may limit its ability to learn highly abstract and complex features.\n",
    "3.Inefficient with Large Images: LeNet-5's fixed input size of 32x32 pixels makes it less suitable for larger \n",
    "images that require higher resolution or more extensive receptive fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33fc45c-3a39-40de-ad7a-866b1b9941c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 5s 10ms/step - loss: 0.4191 - accuracy: 0.8770 - val_loss: 0.1280 - val_accuracy: 0.9608\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.1171 - accuracy: 0.9646 - val_loss: 0.0838 - val_accuracy: 0.9760\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0862 - accuracy: 0.9730 - val_loss: 0.0783 - val_accuracy: 0.9780\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0670 - accuracy: 0.9792 - val_loss: 0.0666 - val_accuracy: 0.9820\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0557 - accuracy: 0.9826 - val_loss: 0.0598 - val_accuracy: 0.9838\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0495 - accuracy: 0.9845 - val_loss: 0.0634 - val_accuracy: 0.9835\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0411 - accuracy: 0.9873 - val_loss: 0.0531 - val_accuracy: 0.9832\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.0437 - val_accuracy: 0.9862\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0345 - accuracy: 0.9887 - val_loss: 0.0432 - val_accuracy: 0.9890\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.0498 - val_accuracy: 0.9867\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0402 - accuracy: 0.9870\n",
      "Test Loss: 0.040209293365478516\n",
      "Test Accuracy: 0.9869999885559082\n",
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "# Q4 Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its \n",
    "# perfocmance and provide insights.\n",
    "# ans:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "num_classes = 10\n",
    "\n",
    "# Create the LeNet-5 architecture using the Sequential API\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Optionally, perform predictions on new unseen data\n",
    "predictions = model.predict(x_test[:10])\n",
    "# print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c635353-23cb-453c-a91e-469086cd9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC: Analyzing AlexNet\n",
    "Q1 Present an overview of the AlexNet architecture.\n",
    "ans:\n",
    "AlexNet is a deep convolutional neural network (CNN) architecture that achieved significant breakthrough performance in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. The key components and overview of the AlexNet architecture are as follows:\n",
    "\n",
    "Input Layer: AlexNet takes a color image of size 227x227 pixels as input.\n",
    "\n",
    "Convolutional Layers: AlexNet begins with five convolutional layers. The first convolutional layer uses 96 \n",
    "filters with a kernel size of 11x11 and a stride of 4. The second and fifth convolutional layers use 256 \n",
    "filters with a kernel size of 5x5. The third and fourth convolutional layers use 384 and 384 filters with a\n",
    "kernel size of 3x3, respectively. These layers perform feature extraction by applying convolution operations \n",
    "on the input image.\n",
    "\n",
    "Activation Function: After each convolutional layer, AlexNet applies the Rectified Linear Unit (ReLU) \n",
    "activation function, introducing non-linearity and enabling the network to learn complex representations.\n",
    "\n",
    "Pooling Layers: Following the activation function, AlexNet has three max pooling layers with a kernel size of \n",
    "3x3 and a stride of 2. Pooling reduces the spatial dimensions and captures the most important features.\n",
    "\n",
    "Local Response Normalization (LRN): LRN is applied after the first and second convolutional layers. It \n",
    "normalizes the responses within local neighborhoods, enhancing the contrast between features and improving\n",
    "the model's generalization.\n",
    "\n",
    "Fully Connected Layers: AlexNet concludes with three fully connected layers. The first two fully connected \n",
    "layers have 4096 units, and the last layer serves as the output layer with 1000 units \n",
    "(corresponding to the 1000 classes in the ImageNet dataset). These layers perform classification based on the\n",
    "learned features.\n",
    "\n",
    "Dropout: Dropout regularization is applied to the fully connected layers, randomly dropping out some of the \n",
    "units during training to prevent overfitting.\n",
    "\n",
    "Softmax Activation: The output layer of AlexNet uses the softmax activation function to produce a probability\n",
    "distribution over the 1000 classes, indicating the predicted probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde1bdc-96be-46c3-807a-9521e7ccc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough\n",
    "performance.\n",
    "ans:\n",
    "The architectural innovations introduced in AlexNet that contributed to its breakthrough performance are as follows:\n",
    "\n",
    "Deep Architecture: AlexNet was one of the first CNN architectures to have a deeper structure compared to previous models. The increased depth allowed for better feature representation and extraction, enabling the network to learn more abstract and complex features.\n",
    "\n",
    "Large-Scale Convolutional Layers: AlexNet introduced large-scale convolutional layers, such as the 11x11 and 5x5 filters. These larger filters helped capture higher-level features and enabled the network to learn more spatially spread-out patterns.\n",
    "\n",
    "Overlapping Pooling: AlexNet used overlapping pooling, where the pooling regions had a stride smaller than the size of the pooling kernel. This allowed for more spatially dense pooling and helped retain more spatial information in the output feature maps.\n",
    "\n",
    "GPU Acceleration: AlexNet utilized the power of Graphics Processing Units (GPUs) for training, which significantly accelerated the training process and enabled the training of larger and deeper models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53009f-6c03-49a5-9af8-e6b3a4bf6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
    "ans:\n",
    " In AlexNet, the convolutional layers are responsible for feature extraction by applying filters and convolutions to the input image. These layers learn various low-level and high-level features as the depth increases.\n",
    "\n",
    "Pooling layers, specifically max pooling in AlexNet, reduce the spatial dimensions of the feature maps while retaining the most salient information. They downsample the feature maps, introducing invariance to small spatial translations and improving computational efficiency.\n",
    "\n",
    "Fully connected layers in AlexNet serve as the classification layers. They take the learned features from the convolutional and pooling layers and perform high-level reasoning and decision-making. The fully connected layers connect all the neurons from the previous layer to every neuron in the current layer, allowing the model to learn complex relationships and make class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330b158-78c8-4a65-b441-680282a74b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 13:02:01.742500: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-16 13:02:02.401696: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-16 13:02:02.405565: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 13:02:04.283861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-16 13:02:08.600404: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 30917400000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Q4 Implement AlexNet using a deep learning fcamework oj your choice and evaluate its performance\n",
    "# on a dataset of your choice.\n",
    "# ans:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the dataset (CIFAR-10 in this example)\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = tf.image.resize(x_train, (227, 227)) / 255.0\n",
    "x_test = tf.image.resize(x_test, (227, 227)) / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Create the AlexNet architecture using the Sequential API\n",
    "model = Sequential()\n",
    "model.add(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(227, 227, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(256, kernel_size=(5, 5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Optionally, perform predictions on new unseen data\n",
    "predictions = model.predict(x_test[:10])\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a05e3-b047-4fa1-9acb-0521aa180176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
